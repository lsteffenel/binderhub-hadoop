{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## MapReduce WordCount en Java\n\nComme présenté dans le cours, Hadoop MapReduce a été initialement développé en Java et pour cette raison utilise préférentiellement du code source en Java. \n\nDans ce premier exercice, vous allez apprendre à écrire et compiler un code pour compter les mots des livres de notre dataset.\n\nTout d'abord, vous allez compléter le code de chacun des fichiers avec le code indiqué ci-dessus. Pour cela, ouvrez chacun de ces fichiers à partir de la racine du dashboard (vous pouvez y retourner en cliquant sur le logo \"jupyterhub\" en haut à gauche), et copiez le code des paragraphes respectifs :\n \n* urca/bigdata/WordCount.java\n* urca/bigdata/TokenizerMapper.java\n* urca/bigdata/IntSumReducer.java\n\n`ATTENTION : ces fichiers sont vides, c'est à vous d'éditer les fichiers et remplir avec le contenu ci-dessous`.\n\n### Etape 1 : créer un \"Driver\"\n\nOn appelle **driver** la classe qui est appellée en premier lors de l'exécution d'un programme. Dans notre cas, cette classe aura le rôle d'initialiser l'environnement Hadoop et de coordonner qui sera appelé à chaque étape (Map, Reduce). \n\nDans le paragraphe suivant vous pouvez voir le code source pour le \"driver\" du fichier `WordCount.java`. ","metadata":{}},{"cell_type":"raw","source":"package urca.bigdata;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(IntSumReducer.class);\n        job.setReducerClass(IntSumReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}","metadata":{}},{"cell_type":"markdown","source":"Cette classe (qui appartient au package \"urca.bigdata\") se connecte à l'environnement Hadoop en créant un `Job` \"**word count**\". Par la suite, elle renseigne certaines informations utiles au déroulement du programme : \n* quelle sera la classe utilisée pour le map --> `job.setMapperClass(TokenizerMapper.class);`\n* quelle sera la classe utilisée pour le reduce --> `job.setReducerClass(IntSumReducer.class);`\n* quelle sera la classe utilisée pour le combiner (optimisation locale) --> `job.setCombinerClass(IntSumReducer.class);`\n* quel est le format de la clé produite à la sortie (texte) --> `job.setOutputKeyClass(Text.class);`\n* quel est le format de la valeur produite à la sortie (entier) --> `job.setOutputValueClass(IntWritable.class);`\n* le point d'entrée pour la lecture des données à traiter (chemin spécifié dans le premier argument d'appel) --> `FileInputFormat.addInputPath(job, new Path(args[0]));`\n* l'endroi où les résultats seront enregistrés (deuxième argument d'appel) --> `FileOutputFormat.setOutputPath(job, new Path(args[1]));`\n\nOn voit donc que ce **driver** fait appel à d'autres classes. Regardons tout d'abord la classe *TokenizerMapper.java* :","metadata":{}},{"cell_type":"raw","source":"package urca.bigdata;\n\n  import org.apache.hadoop.io.IntWritable;\n  import org.apache.hadoop.io.Text;\n  import org.apache.hadoop.mapreduce.Mapper;\n\n  import java.io.IOException;\n  import java.util.StringTokenizer;\n\n  public class TokenizerMapper\n        extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Mapper.Context context\n    ) throws IOException, InterruptedException {\n        StringTokenizer itr = new StringTokenizer(value.toString());\n        while (itr.hasMoreTokens()) {\n            word.set(itr.nextToken());\n            context.write(word, one);\n        }\n    }\n  }","metadata":{}},{"cell_type":"markdown","source":"Cette classe reçoit de Hadoop une ligne de texte avec une clé en format *Object*. Comme nous voulons compter les mots, la méthode `map()` effectue une *tokenisation*, c'est à dire, elle découpe la ligne en mots. \n\nPour chaque mot produit, une paire **clé, valeur** sera produite (`context.write(word, one)`):\n* *word* contient le mot trouvé\n* *one* est un entier **1**\n\nL'ensemble des ces clés/valeurs passera d'abord par une optimisation locale (*combiner*) qui regroupera tous les mots identiques dans une machine, puis sera transmis au *Reducer* pour produire le comptage final. Comme l'opération combiner et reducer est la même (la seule chose qui change est l'étendu de l'opération) on utilise la même classe `IntSumReducer.java` :\n","metadata":{}},{"cell_type":"raw","source":"package urca.bigdata;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class IntSumReducer\n        extends Reducer<Text,IntWritable,Text,IntWritable> {\n\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,\n                       Context context\n    ) throws IOException, InterruptedException {\n        int sum = 0;\n        for (IntWritable val : values) {\n            //System.out.println(\"value: \"+val.get());\n            sum += val.get();\n        }\n        //System.out.println(\"--> Sum = \"+sum);\n        result.set(sum);\n        context.write(key, result);\n    }\n}","metadata":{}},{"cell_type":"markdown","source":"Ici, la méthode `reduce()` reçoit des ensembles {clé, {liste de valeurs}}. En parcourant la liste, cette méthode récupère les valeurs et les additione, afin de produire une clé finale contenant **mot, somme**. ","metadata":{}},{"cell_type":"markdown","source":"## Compiler et exécuter\n\n\nC'est bon, les fichiers sont remplis et sauvegardés ? C'est l'heure de les compiler et produire un **jar** pour Hadoop\n* si vous voulez, vous pouvez ouvrir un terminal à partir de la racine du dashboard (new->Terminal) et exécuter ces commandes vous même.","metadata":{}},{"cell_type":"code","source":"## mise en place d'un répertoire pour le code compilé\n! mkdir WordCount\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## mise en place des variables d'environnement et compilation\n! javac -classpath `hadoop-2.10.1/bin/hadoop classpath`:. -d WordCount urca/bigdata/WordCount.java","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## création du jar\n! jar -cvf WC.jar -C WordCount/ .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exécution\n\nVoilà, tout est prêt. On a un Jar avec les exécutables qui Hadoop peut utiliser pour déployer l'application. On a le dataset dans HDFS. Il ne reste qu'à lancer le code.\n\nPour cela, on va appeler Hadoop avec certains paramètres que vous devez pouvoir comprendre facilement : \n\n`hadoop jar WC.jar urca.bigdata.WordCount livre countMots`\n\nOn dit à hadoop de lancer le jar *WC.jar* et d'appeler la classe *urca.bigdata.WordCount*.\nEnsuite, on passe comme paramètres deux répertoires de HDFS :\n* le répertoire *livres* qui contient notre dataset\n* le répertoire *countMots* qui contiendra le résultat\n\n### ATTENTION : le répertoire pour les résultats ne doit pas être crée avant : Hadoop se refuse d'écraser un répertoire existant \n\nQuand vous lancerez la commande dans le paragraphe ci-dessous, vous verez la progression du calcul. \n","metadata":{}},{"cell_type":"code","source":"! hadoop-2.10.1/bin/hadoop jar WC.jar urca.bigdata.WordCount livres countMots","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pour finir, on peut regarder le résultat final stocké dans HDFS (et même le rappatrier) :","metadata":{}},{"cell_type":"code","source":"! hadoop-2.10.1/bin/hdfs dfs -ls countMots","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! hadoop-2.10.1/bin/hdfs dfs -cat countMots/part-r-00000 | tail -200","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On affiche une petite partie du résultat, mais vous pouvez observer qu'il y a encore du travail à faire pour rendre un résultat propre. Par exemple, on a des signes de ponctuation associés aux mots (*walk?, walk!*) les font compter pour des mots différents. Le tokenizer pourrait filtrer ces signes afin de ne produire que des mots \"propres\".\n\nCet exercice est fini, passons à l'activité 3.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}