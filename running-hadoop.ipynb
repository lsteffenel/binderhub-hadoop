{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Initial definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_VERSION=2.9.2\n",
      "env: HADOOP_PATH=hadoop-2.9.2\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_VERSION     2.9.2\n",
    "%env HADOOP_PATH hadoop-2.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Downloading Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://ftp.unicamp.br/pub/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -q --show-progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Extracting compressed files and removing .tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf hadoop-${HADOOP_VERSION}.tar.gz >/dev/null \n",
    "# !rm       hadoop-${HADOOP_VERSION}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Discovering the Java path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "!dirname $(dirname $(readlink -f $(which javac)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Java path envvar\n",
    "\n",
    "We also added it to user's .bashrc so it will be loaded as the nodes perform ssh connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \" > .bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Hadoop in Standalone Mode (local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## MapReduce in the local filesystem - word count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/07 05:14:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/07/07 05:14:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/07/07 05:14:11 INFO input.FileInputFormat: Total input files to process : 1\n",
      "19/07/07 05:14:11 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/07/07 05:14:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1543156223_0001\n",
      "19/07/07 05:14:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/07/07 05:14:11 INFO mapreduce.Job: Running job: job_local1543156223_0001\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/07/07 05:14:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/07/07 05:14:11 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: Starting task: attempt_local1543156223_0001_m_000000_0\n",
      "19/07/07 05:14:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/07/07 05:14:11 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/07/07 05:14:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: Processing split: file:/home/matheus/local-home/git/github/binderhub-hadoop/resources/examples/newyorknewyork.txt:0+865\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: \n",
      "19/07/07 05:14:11 INFO mapred.MapTask: Starting flush of map output\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: Spilling map output\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: bufstart = 0; bufend = 1625; bufvoid = 104857600\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213640(104854560); length = 757/6553600\n",
      "19/07/07 05:14:11 INFO mapred.MapTask: Finished spill 0\n",
      "19/07/07 05:14:11 INFO mapred.Task: Task:attempt_local1543156223_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: map\n",
      "19/07/07 05:14:11 INFO mapred.Task: Task 'attempt_local1543156223_0001_m_000000_0' done.\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local1543156223_0001_m_000000_0\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/07/07 05:14:11 INFO mapred.LocalJobRunner: Starting task: attempt_local1543156223_0001_r_000000_0\n",
      "19/07/07 05:14:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/07/07 05:14:11 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/07/07 05:14:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/07/07 05:14:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@71f74075\n",
      "19/07/07 05:14:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/07/07 05:14:12 INFO reduce.EventFetcher: attempt_local1543156223_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/07/07 05:14:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1543156223_0001_m_000000_0 decomp: 890 len: 894 to MEMORY\n",
      "19/07/07 05:14:12 INFO reduce.InMemoryMapOutput: Read 890 bytes from map-output for attempt_local1543156223_0001_m_000000_0\n",
      "19/07/07 05:14:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 890, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->890\n",
      "19/07/07 05:14:12 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "19/07/07 05:14:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/07/07 05:14:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/07/07 05:14:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/07/07 05:14:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/07/07 05:14:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 884 bytes\n",
      "19/07/07 05:14:12 INFO reduce.MergeManagerImpl: Merged 1 segments, 890 bytes to disk to satisfy reduce memory limit\n",
      "19/07/07 05:14:12 INFO reduce.MergeManagerImpl: Merging 1 files, 894 bytes from disk\n",
      "19/07/07 05:14:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/07/07 05:14:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/07/07 05:14:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 884 bytes\n",
      "19/07/07 05:14:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/07/07 05:14:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/07/07 05:14:12 INFO mapred.Task: Task:attempt_local1543156223_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/07/07 05:14:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/07/07 05:14:12 INFO mapred.Task: Task attempt_local1543156223_0001_r_000000_0 is allowed to commit now\n",
      "19/07/07 05:14:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1543156223_0001_r_000000_0' to file:/home/matheus/local-home/git/github/binderhub-hadoop/output/_temporary/0/task_local1543156223_0001_r_000000\n",
      "19/07/07 05:14:12 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "19/07/07 05:14:12 INFO mapred.Task: Task 'attempt_local1543156223_0001_r_000000_0' done.\n",
      "19/07/07 05:14:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local1543156223_0001_r_000000_0\n",
      "19/07/07 05:14:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/07/07 05:14:12 INFO mapreduce.Job: Job job_local1543156223_0001 running in uber mode : false\n",
      "19/07/07 05:14:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/07/07 05:14:12 INFO mapreduce.Job: Job job_local1543156223_0001 completed successfully\n",
      "19/07/07 05:14:12 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=610628\n",
      "\t\tFILE: Number of bytes written=1554623\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=36\n",
      "\t\tMap output records=190\n",
      "\t\tMap output bytes=1625\n",
      "\t\tMap output materialized bytes=894\n",
      "\t\tInput split bytes=160\n",
      "\t\tCombine input records=190\n",
      "\t\tCombine output records=80\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce shuffle bytes=894\n",
      "\t\tReduce input records=80\n",
      "\t\tReduce output records=80\n",
      "\t\tSpilled Records=160\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=486539264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=865\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=587\n"
     ]
    }
   ],
   "source": [
    "!${HADOOP_PATH}/bin/hadoop jar ${HADOOP_PATH}/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar  \\\n",
    "                               wordcount \\\n",
    "                               ./resources/examples/newyorknewyork.txt \\\n",
    "                               ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing files in the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-r-00000  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls ./output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And\t3\n",
      "Come\t1\n",
      "Head\t1\n",
      "I\t8\n",
      "I'll\t1\n",
      "I'm\t3\n",
      "If\t2\n",
      "In\t1\n",
      "It's\t1\n",
      "King\t1\n",
      "My\t1\n",
      "New\t13\n",
      "Right\t2\n",
      "Start\t1\n",
      "That\t2\n",
      "These\t2\n",
      "They\t3\n",
      "Top\t2\n",
      "York\t13\n",
      "You\t1\n",
      "a\t3\n",
      "about\t2\n",
      "all\t1\n",
      "am\t2\n",
      "anywhere\t2\n",
      "are\t3\n",
      "away\t2\n",
      "baby\t1\n",
      "be\t1\n",
      "bet\t1\n",
      "blues\t2\n",
      "brand\t2\n",
      "can\t2\n",
      "city\t2\n",
      "come\t1\n",
      "doesn't\t1\n",
      "find\t2\n",
      "gonna\t2\n",
      "have\t1\n",
      "heap\t2\n",
      "heart\t1\n",
      "hill\t3\n",
      "in\t3\n",
      "it\t8\n",
      "just\t1\n",
      "king\t2\n",
      "know\t1\n",
      "leaving\t1\n",
      "list\t1\n",
      "little\t2\n",
      "longing\t1\n",
      "make\t6\n",
      "melted\t1\n",
      "melting\t1\n",
      "never\t1\n",
      "new\t2\n",
      "news\t1\n",
      "of\t10\n",
      "old\t2\n",
      "on\t1\n",
      "part\t1\n",
      "shoes\t1\n",
      "sleep\t1\n",
      "sleeps\t1\n",
      "spreading\t1\n",
      "start\t2\n",
      "stray\t1\n",
      "that\t2\n",
      "the\t8\n",
      "there\t3\n",
      "through\t2\n",
      "to\t6\n",
      "today\t1\n",
      "town\t2\n",
      "up\t3\n",
      "vagabond\t1\n",
      "very\t1\n",
      "wake\t2\n",
      "want\t3\n",
      "you\t2\n"
     ]
    }
   ],
   "source": [
    "! cat ./output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Hadoop in Pseudo-Distributed Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting sshd server\n",
    "\n",
    "Check `/binder/postBuild` and `/resources/configs/ssh/sshd_config` files for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/sbin/sshd -f resources/configs/ssh/sshd_config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding names to know hosts \n",
    "\n",
    "Commands below stablish ssh connections to used host names/ips. This step avoids yes/no host confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added '[localhost]:8822' (ECDSA) to the list of known hosts.\n",
      "Warning: Permanently added '[127.0.0.1]:8822' (ECDSA) to the list of known hosts.\n",
      "Warning: Permanently added '[0.0.0.0]:8822' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "!ssh -o \"StrictHostKeyChecking no\" $USER@localhost -p 8822 -C \"exit\" \n",
    "!ssh -o \"StrictHostKeyChecking no\" $USER@127.0.0.1 -p 8822 -C \"exit\"\n",
    "!ssh -o \"StrictHostKeyChecking no\" $USER@0.0.0.0   -p 8822 -C \"exit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ssh options to Hadoop via envvar\n",
    "\n",
    "* connecting in a diferent port (`-p 8822`)\n",
    "* avoiding host key checking (`-o StrictHostKeyChecking=no`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_SSH_OPTS=-o StrictHostKeyChecking=no -p 8822\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_SSH_OPTS= -o StrictHostKeyChecking=no -p 8822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying configurations files to Hadoop folder\n",
    "\n",
    "Check the configuration files accordingly to the Hadoop version. \n",
    "Refer to the `/resources/configs/hadoop/<version>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp resources/configs/hadoop/${HADOOP_VERSION}/core-site.xml   ${HADOOP_PATH}/etc/hadoop/\n",
    "!cp resources/configs/hadoop/${HADOOP_VERSION}/hdfs-site.xml   ${HADOOP_PATH}/etc/hadoop/\n",
    "!cp resources/configs/hadoop/${HADOOP_VERSION}/mapred-site.xml ${HADOOP_PATH}/etc/hadoop/\n",
    "!cp resources/configs/hadoop/${HADOOP_VERSION}/yarn-site.xml   ${HADOOP_PATH}/etc/hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Formatting the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/07 05:14:43 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = 47958a0ef968/172.17.0.2\n",
      "STARTUP_MSG:   args = [-format, -force, -nonInteractive]\n",
      "STARTUP_MSG:   version = 2.9.2\n",
      "STARTUP_MSG:   classpath = /home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/etc/hadoop:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/xmlenc-0.52.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/junit-4.11.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/activation-1.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-digester-1.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jersey-core-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jetty-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jettison-1.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-cli-1.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/guava-11.0.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jersey-json-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-lang-2.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/asm-3.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/gson-2.2.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/xz-1.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/hadoop-annotations-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/paranamer-2.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jsch-0.1.54.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jsp-api-2.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/log4j-1.2.17.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-codec-1.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/servlet-api-2.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/hadoop-auth-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-io-2.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/avro-1.7.7.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/commons-net-3.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/jersey-server-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/hadoop-nfs-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/hadoop-common-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/common/hadoop-common-2.9.2-tests.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/asm-3.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-2.9.2-tests.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.2-tests.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.2-tests.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-client-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/hdfs/hadoop-hdfs-client-2.9.2-tests.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/activation-1.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jettison-1.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/guice-3.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/asm-3.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/javax.inject-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/xz-1.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/fst-2.50.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-common-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-router-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-common-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-client-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-api-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-registry-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar:/home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.2.jar:/contrib/capacity-scheduler/*.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 826afbeae31ca687bc2f8471dc841b66ed2c6704; compiled by 'ajisaka' on 2018-11-13T12:42Z\n",
      "STARTUP_MSG:   java = 1.8.0_212\n",
      "************************************************************/\n",
      "19/07/07 05:14:44 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "19/07/07 05:14:44 INFO namenode.NameNode: createNameNode [-format, -force, -nonInteractive]\n",
      "Formatting using clusterid: CID-0e2e057a-a1c2-4f43-b9af-8db0b36aca30\n",
      "19/07/07 05:14:44 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: fsOwner             = matheus (auth:SIMPLE)\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "19/07/07 05:14:44 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "19/07/07 05:14:44 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "19/07/07 05:14:44 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Jul 07 05:14:44\n",
      "19/07/07 05:14:44 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "19/07/07 05:14:44 INFO util.GSet: VM type       = 64-bit\n",
      "19/07/07 05:14:44 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\n",
      "19/07/07 05:14:44 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "19/07/07 05:14:44 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\n",
      "19/07/07 05:14:44 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "19/07/07 05:14:44 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "19/07/07 05:14:44 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "19/07/07 05:14:44 INFO util.GSet: Computing capacity for map INodeMap\n",
      "19/07/07 05:14:44 INFO util.GSet: VM type       = 64-bit\n",
      "19/07/07 05:14:44 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\n",
      "19/07/07 05:14:44 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "19/07/07 05:14:44 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "19/07/07 05:14:44 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "19/07/07 05:14:44 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "19/07/07 05:14:44 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\n",
      "19/07/07 05:14:44 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "19/07/07 05:14:44 INFO util.GSet: VM type       = 64-bit\n",
      "19/07/07 05:14:44 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\n",
      "19/07/07 05:14:44 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "19/07/07 05:14:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "19/07/07 05:14:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "19/07/07 05:14:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "19/07/07 05:14:44 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "19/07/07 05:14:44 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "19/07/07 05:14:44 INFO util.GSet: VM type       = 64-bit\n",
      "19/07/07 05:14:44 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\n",
      "19/07/07 05:14:44 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "19/07/07 05:14:44 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1210192475-172.17.0.2-1562476484405\n",
      "19/07/07 05:14:44 INFO common.Storage: Storage directory /tmp/hadoop-matheus/dfs/name has been successfully formatted.\n",
      "19/07/07 05:14:44 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-matheus/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "19/07/07 05:14:44 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-matheus/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 326 bytes saved in 0 seconds .\n",
      "19/07/07 05:14:44 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "19/07/07 05:14:44 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at 47958a0ef968/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!${HADOOP_PATH}/bin/hdfs namenode -format -force -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting DFS (NameNode, SecondaryNameNode, and DataNode daemons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/logs/hadoop-matheus-namenode-47958a0ef968.out\n",
      "localhost: starting datanode, logging to /home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/logs/hadoop-matheus-datanode-47958a0ef968.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /home/matheus/local-home/git/github/binderhub-hadoop/hadoop-2.9.2/logs/hadoop-matheus-secondarynamenode-47958a0ef968.out\n",
      "743 DataNode\n",
      "617 NameNode\n",
      "1035 Jps\n",
      "908 SecondaryNameNode\n"
     ]
    }
   ],
   "source": [
    "!${HADOOP_PATH}/sbin/start-dfs.sh\n",
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## MapReduce - Word count example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating folders in the distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HADOOP_PATH}/bin/hdfs dfs -mkdir /user/\n",
    "!${HADOOP_PATH}/bin/hdfs dfs -mkdir /user/matheus/\n",
    "!${HADOOP_PATH}/bin/hdfs dfs -mkdir /user/matheus/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying a file to a folder in the distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HADOOP_PATH}/bin/hdfs dfs -put ./resources/examples/newyorknewyork.txt /user/matheus/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing files in a folder of the distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 matheus supergroup        865 2019-07-07 05:16 /user/matheus/input/newyorknewyork.txt\n"
     ]
    }
   ],
   "source": [
    "!${HADOOP_PATH}/bin/hdfs dfs -ls /user/matheus/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the contents of a file in the distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start spreading the news\n",
      "I am leaving today\n",
      "I want to be a part of it\n",
      "New York New York\n",
      "These vagabond shoes\n",
      "They are longing to stray\n",
      "Right through the very heart of it\n",
      "New York New York\n",
      "I want to wake up in that city\n",
      "That doesn't sleep\n",
      "And find I'm king of the hill\n",
      "Top of the heap\n",
      "My little town blues\n",
      "They are melting away\n",
      "I gonna make a brand new start of it\n",
      "In old New York\n",
      "If I can make it there\n",
      "I'll make it anywhere\n",
      "It's up to you\n",
      "New York New York\n",
      "New York New York\n",
      "I want to wake up in that city\n",
      "That never sleeps\n",
      "And find I'm king of the hill\n",
      "Top of the list\n",
      "Head of the heap\n",
      "King of the hill\n",
      "These are little town blues\n",
      "They have all melted away\n",
      "I am about to make a brand new start of it\n",
      "Right there in old New York\n",
      "And you bet baby\n",
      "If I can make it there\n",
      "You know I'm gonna make it just about anywhere\n",
      "Come on come through\n",
      "New York New York New York\n"
     ]
    }
   ],
   "source": [
    "!${HADOOP_PATH}/bin/hdfs dfs -cat /user/matheus/input/newyorknewyork.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Running MapReduce job in Pseudo-Distributed Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/07 05:16:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/07/07 05:16:29 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:30 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:31 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:32 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:33 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:34 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:35 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:36 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:37 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:38 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:39 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:40 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:41 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:42 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:43 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:44 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:45 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:46 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:47 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:48 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "19/07/07 05:16:48 INFO retry.RetryInvocationHandler: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over null after 1 failover attempts. Trying to failover after sleeping for 15754ms.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!${HADOOP_PATH}/bin/hadoop jar ./${HADOOP_PATH}/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar  \\\n",
    "                                 wordcount \\\n",
    "                                 /user/matheus/input /user/matheus/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing files in the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/user/matheus/output/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!./${HADOOP_PATH}/bin/hdfs dfs -ls /user/matheus/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/user/matheus/output/part-r-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!./${HADOOP_PATH}/bin/hdfs dfs -cat /user/matheus/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Hadoop YARN in Pseudo-Distributed Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying configurations files to Hadoop folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp resources/configs/hadoop/${HADOOP_VERSION}/mapred-site.xml ${HADOOP_PATH}/etc/hadoop/\n",
    "!cp resources/configs/hadoop/${HADOOP_VERSION}/yarn-site.xml   ${HADOOP_PATH}/etc/hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HADOOP_PATH}/sbin/start-dfs.sh\n",
    "!${HADOOP_PATH}/sbin/start-yarn.sh\n",
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Running MapReduce job via YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HADOOP_PATH}/bin/yarn jar ./${HADOOP_PATH}/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar  \\\n",
    "                                 wordcount \\\n",
    "                                 /user/matheus/input /user/matheus/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing files in the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./${HADOOP_PATH}/bin/hdfs dfs -ls /user/matheus/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Reading output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./${HADOOP_PATH}/bin/hdfs dfs -cat /user/matheus/output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!${HADOOP_PATH}/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
