{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction à HDFS\n\nDans cet exercice vous allez apprendre à manipuler le système de fichiers HDFS. \n\nHDFS est l'un des éléments de base de Hadoop, car il coordonne le stockage et la distribution des données dans les différents noeuds d'un cluster Hadoop, permettant ainsi aux applications (MapReduce, Spark, etc.) de se concentrer sur le traitement des données.\n\n* ATTENTION : les données stockées sur HDFS ne sont pas directement accessibles par le shell, il faut passer par une application (*hadoop fs*). Vous pouvez comparer cela au stockage de fichiers dans un serveur distant et son accès avec *ssh* et *scp/sftp*.","metadata":{}},{"cell_type":"markdown","source":"### Petite note à propos de cet environnement Jupyter\n\nLes \"paragraphes\" sont censés exécuter des commandes en python. Comme dans cet exercice nous allons utiliser des commandes shell, il faudra toujours utiliser un **!** (point d'exclamation) avant les commandes.\n\nPar exemple **\"! hadoop dfs -ls\"**\n\n","metadata":{}},{"cell_type":"markdown","source":"### Par où on commence ?\n\nCes exercices se trouvent dans une machine virtuelle où chacun est le seul utilisateur. Utilisez les commandes **ls**, **pwd**, pour vous localiser dans le *home* de votre machine.\n\n* certaines commandes de base du shell (ls, cat, rm) n'ont pas besoin de l'exclamation. À l'inverse, la commande **cd** doit obligatoirement utiliser un percent (**%cd**). Dans le doute, il sera plus simple de précéder ces commandes de base avec %. ","metadata":{}},{"cell_type":"code","source":"! ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ls -lh ~/resources/datasets/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit déjà quelques *datasets*, mais ce n'est pas ce qu'on souhaite pour ce TP (on les utilisera autre fois). \nOn va donc télécharger deux livres très connus en sciences et deux livre classiques de litérature.","metadata":{}},{"cell_type":"code","source":"! wget http://urca.lsteffenel.fr/livres.tar.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! tar -xvzf livres.tar.gz\n! mv livres ~/resources/datasets/\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! head -10 ~/resources/datasets/livres/Charles_Darwin___On_the_Origin_of_Species_1st_Edition.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maintenant qu'on a notre *dataset* dans le répertoire `/home/jovyan/resources/datasets`, il faudra le stocker dans HDFS avant de procéder à leur analyse.\n\nTout d'abord, nous allons créer un répertoire dans HDFS afin de déposer ces fichiers. \n\nHDFS est organisé \"presque\" comme un répertoire Linux (plus exactement, il suit le standard POSIX). Les fichiers d'un utilisateur se retrouvent donc sous le répertoire `/user/`, comme vous pouvez voir ci-dessous : ","metadata":{}},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -ls /","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -ls /home/$USER","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bien sûr, pour le moment le répertoire `/home/jovyan` est vide (la variable `$USER` retourne le nom de l'utilisateur par défaut dans Jupyter).\n\nOn voit aussi que, pour accéder à HDFS, nous devons utiliser la commande `hdfs dfs`\n\nCommençons par créer un répertoire pour notre dataset de livres :","metadata":{}},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -mkdir /home/$USER/livres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensuite, nous allons copier le fichier du répertoire `~/resources/datasets/livre/` dans le répertoire `/user/jovyan/livre` grâce à la commande **hfds dfs --put** : \n","metadata":{}},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -put ~/resources/datasets/livres/* /home/$USER/livres/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -ls -h /home/$USER/livres/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Si on n'indique pas le chemin absolu, hdfs utilisera automatiquement le sous-répertoire relatif à l'utilisateur : ","metadata":{}},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -ls -h livres/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Et voilà, le dataset se trouve dans HDFS. Avant de traiter les fichiers, nous pouvons regarder quelques autres commandes pour les manipuler.\n\nTout d'abord, on peut afficher le contenu d'un fichier avec l'option `-cat`: ","metadata":{}},{"cell_type":"code","source":"! ${HADOOP_PATH}/bin/hdfs dfs -cat livres/Charles_Darwin___On_the_Origin_of_Species_1st_Edition.txt | head -n 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En effet, on peut faire le pipeline entre la sortie de `hdfs dfs -cat` et d'autres commandes Linux telles que `head`, `tail`, `grep`, ... ","metadata":{}},{"cell_type":"markdown","source":"### HDFS est fait, passons à Hadoop\n\nVous avez injecté le dataset dans HDFS. Maintenant, on va regarder dans le prochain notebook ([2-IntroMapReduce](./2-IntroHadoopMR.ipynb)) comment l'application `WordCount` est écrite en Java (et comment la compiler et exécuter), pour plus tard effectuer la même chose avec Python.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}