{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc-autonumbering":true,"toc-showtags":false},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial definitions","metadata":{"toc-hr-collapsed":false}},{"cell_type":"code","source":"%env HADOOP_VERSION     2.10.1\n%env HADOOP_PATH hadoop-2.10.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the environment","metadata":{"toc-hr-collapsed":false}},{"cell_type":"markdown","source":"## Downloading Hadoop","metadata":{"toc-hr-collapsed":false}},{"cell_type":"code","source":"!wget http://ftp.unicamp.br/pub/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -q --show-progress","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting compressed files and removing .tar","metadata":{"toc-hr-collapsed":false}},{"cell_type":"code","source":"# !rm ${HADOOP_PATH} -r\n!tar -xvf hadoop-${HADOOP_VERSION}.tar.gz >/dev/null \n!rm       hadoop-${HADOOP_VERSION}.tar.gz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discovering the Java path","metadata":{"toc-hr-collapsed":false}},{"cell_type":"code","source":"!dirname $(dirname $(readlink -f $(which javac)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting the Java path envvar\n\nWe also added it to user's .bashrc so it will be loaded as the nodes perform ssh connections.","metadata":{}},{"cell_type":"code","source":"%env JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \" >> ~/.bashrc\n!echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \" >> ~/.profile\n!echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \" >> ${HADOOP_PATH}/etc/hadoop/hadoop-env.sh","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hadoop in Standalone Mode (local)","metadata":{"toc-hr-collapsed":false}},{"cell_type":"markdown","source":"## MapReduce in the local filesystem - word count example","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"!${HADOOP_PATH}/bin/hadoop jar ${HADOOP_PATH}/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar wordcount \\\n                               ./resources/examples/newyorknewyork.txt ./output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Listing files in the output folder","metadata":{}},{"cell_type":"code","source":"!ls ./output/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading output file","metadata":{}},{"cell_type":"code","source":"! cat ./output/part-r-00000","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ARRETER ICI ET PASSER À L'EXERCICE 1-IntroHDFS\n\nBinder semble être trop sollicité et même si la capacité théorique de chaque noeud est de 2Go de mémoire, il tend à arrêter les serveurs si ça dépasse les 300-400Mo d'usage. Dans ce cas, il vaut mieux continuer à apprendre en mode \"Standalone\".\n\n# %%%%%%%%%%%%%%%%%%%%","metadata":{}},{"cell_type":"markdown","source":"# Hadoop in Pseudo-Distributed Mode","metadata":{"toc-hr-collapsed":false}},{"cell_type":"markdown","source":"## Preparing the environment","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"### Starting sshd server\n\nCheck `/binder/postBuild` and `/resources/configs/ssh/sshd_config` files for more details","metadata":{}},{"cell_type":"code","source":"!/usr/sbin/sshd -f resources/configs/ssh/sshd_config ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding names to know hosts \n\nCommands below stablish ssh connections to used host names/ips. This step avoids yes/no host confirmation.","metadata":{}},{"cell_type":"code","source":"!ssh -o \"StrictHostKeyChecking no\" $USER@localhost -p 8822 -C \"exit\" \n!ssh -o \"StrictHostKeyChecking no\" $USER@0.0.0.0   -p 8822 -C \"exit\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding ssh options to Hadoop via envvar\n\n* connecting in a diferent port (`-p 8822`)\n* avoiding host key checking (`-o StrictHostKeyChecking=no`)","metadata":{}},{"cell_type":"code","source":"%env HADOOP_SSH_OPTS= -o StrictHostKeyChecking=no -p 8822","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env PDSH_RCMD_TYPE ssh","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Copying configurations files to Hadoop folder\n\nCheck the configuration files accordingly to the Hadoop version. \nRefer to the `/resources/configs/hadoop/<version>`.","metadata":{}},{"cell_type":"code","source":"!cp resources/configs/hadoop/${HADOOP_VERSION}/core-site.xml   ${HADOOP_PATH}/etc/hadoop/\n!cp resources/configs/hadoop/${HADOOP_VERSION}/hdfs-site.xml   ${HADOOP_PATH}/etc/hadoop/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Formatting the filesystem","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"!${HADOOP_PATH}/bin/hdfs namenode -format -force -nonInteractive","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Starting DFS (NameNode, SecondaryNameNode, and DataNode daemons)","metadata":{}},{"cell_type":"code","source":"!${HADOOP_PATH}/sbin/start-dfs.sh\n!jps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MapReduce - Word count example ","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"### Creating folders in the distributed file system","metadata":{}},{"cell_type":"code","source":"!${HADOOP_PATH}/bin/hdfs dfs -mkdir /user/\n!${HADOOP_PATH}/bin/hdfs dfs -mkdir /user/rt0902/\n!${HADOOP_PATH}/bin/hdfs dfs -mkdir /user/rt0902/input/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Copying a file to a folder in the distributed file system","metadata":{}},{"cell_type":"code","source":"!${HADOOP_PATH}/bin/hdfs dfs -put ./resources/examples/newyorknewyork.txt /user/rt0902/input/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Listing files in a folder of the distributed file system","metadata":{}},{"cell_type":"code","source":"!${HADOOP_PATH}/bin/hdfs dfs -ls /user/rt0902/input/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retrieving the contents of a file in the distributed file system","metadata":{}},{"cell_type":"code","source":"!${HADOOP_PATH}/bin/hdfs dfs -cat /user/rt0902/input/newyorknewyork.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Running MapReduce job in Pseudo-Distributed Mode","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"!./${HADOOP_PATH}/bin/hadoop jar  ./${HADOOP_PATH}/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar wordcount \\\n                                /user/rt0902/input /user/rt0902/output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Listing files in the output folder","metadata":{}},{"cell_type":"code","source":"!./${HADOOP_PATH}/bin/hdfs dfs -ls /user/rt0902/output/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading output file","metadata":{}},{"cell_type":"code","source":"!./${HADOOP_PATH}/bin/hdfs dfs -cat /user/rt0902/output/part-r-00000","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Starting YARN in Pseudo-Distributed Mode","metadata":{"toc-hr-collapsed":false}},{"cell_type":"markdown","source":"## Preparing the environment","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"### Copying configurations files to Hadoop folder","metadata":{}},{"cell_type":"code","source":"!cp resources/configs/hadoop/${HADOOP_VERSION}/mapred-site.xml ${HADOOP_PATH}/etc/hadoop/\n!cp resources/configs/hadoop/${HADOOP_VERSION}/yarn-site.xml   ${HADOOP_PATH}/etc/hadoop/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Starting YARN","metadata":{}},{"cell_type":"code","source":"!${HADOOP_PATH}/sbin/start-yarn.sh\n!jps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MapReduce via YARN - Word count example ","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"!./${HADOOP_PATH}/bin/yarn jar  ./${HADOOP_PATH}/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar wordcount \\\n                                /user/rt0902/input /user/rt0902/output2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Listing files in the output folder","metadata":{}},{"cell_type":"code","source":"!./${HADOOP_PATH}/bin/hdfs dfs -ls /user/rt0902/output2/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading output file","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"!./${HADOOP_PATH}/bin/hdfs dfs -cat /user/rt0902/output2/part-r-00000","metadata":{},"execution_count":null,"outputs":[]}]}